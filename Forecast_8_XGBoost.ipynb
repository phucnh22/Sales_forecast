{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from skforecast.model_selection import grid_search_forecaster\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from xgboost import XGBRegressor\n",
    "from sktime.performance_metrics.forecasting import (\n",
    "    mean_absolute_scaled_error,\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_squared_error,\n",
    ")\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from sktime.transformations.series.detrend import Deseasonalizer, Detrender\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(y, X, horizon):\n",
    "    y.index.freq='D'\n",
    "\n",
    "    # split\n",
    "    y_train, y_test = train_test_split(y, test_size=horizon, shuffle=False)\n",
    "\n",
    "    # transform (deseasonalize)\n",
    "    transformer = make_pipeline(Deseasonalizer(sp=7), Detrender())\n",
    "    y_train_trans = transformer.fit_transform(y_train)\n",
    "    y_train_trans.name = y.name\n",
    "    y_test_trans = transformer.transform(y_test)\n",
    "    y_test_trans.name = y.name\n",
    "    y_trans = pd.concat([y_train_trans, y_test_trans])\n",
    "\n",
    "    # join\n",
    "    df = X.join(y_trans).dropna()\n",
    "    \n",
    "    # extract exo. variables from date index\n",
    "    df['dayofweek'] = df.index.dayofweek\n",
    "    df['dayofmonth'] = df.index.day\n",
    "    df['dayofyear'] = df.index.dayofyear\n",
    "    df['weekofyear'] = df.index.isocalendar()['week']\n",
    "    df['month'] = df.index.month\n",
    "    df['quarter'] = df.index.quarter\n",
    "    df['year'] = df.index.year\n",
    "\n",
    "    # rolling mean\n",
    "    df['rolling_mean_2'] = df['sales'].rolling(2).mean()\n",
    "    df['rolling_mean_3'] = df['sales'].rolling(3).mean()\n",
    "    df['rolling_mean_4'] = df['sales'].rolling(4).mean()\n",
    "    df['rolling_mean_5'] = df['sales'].rolling(5).mean()\n",
    "    df['rolling_mean_6'] = df['sales'].rolling(6).mean()\n",
    "    df['rolling_mean_7'] = df['sales'].rolling(7).mean()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    return df, transformer\n",
    "\n",
    "\n",
    "# CV \n",
    "def cross_validation_result(data, tuned_model, model_name, transformer, horizon, rolls):\n",
    "    # score model with CV on store data\n",
    "    mae_CVs = []\n",
    "    rmse_CVs = []\n",
    "    mape_CVs = []\n",
    "    mase_CVs = []\n",
    "    for i in range(rolls):\n",
    "        # print(f\"fold {i}---------------\")\n",
    "        \n",
    "        # split data\n",
    "        y_train = data.iloc[: -(rolls - i) * horizon]\n",
    "        y_test = data.iloc[\n",
    "            np.r_[-(rolls - i) * horizon : -(rolls - i - 1) * horizon]]\n",
    "\n",
    "        # fit model\n",
    "        model = tuned_model\n",
    "        model.fit(\n",
    "            y    = y_train['sales'],\n",
    "            exog = y_train[y_train.columns.difference(['sales'])]\n",
    "        )\n",
    "        \n",
    "        # make forecast\n",
    "        y_hat = model.predict(\n",
    "                        steps = horizon,\n",
    "                        exog = y_test[y_test.columns.difference(['sales'])]\n",
    "                    )\n",
    "        y_hat = pd.Series(data=y_hat, index=y_test.index)\n",
    "        \n",
    "        # inverse\n",
    "        y_train = transformer.inverse_transform(y_train['sales'])\n",
    "        y_test = transformer.inverse_transform(y_test['sales'])\n",
    "        y_hat = transformer.inverse_transform(y_hat)\n",
    "                \n",
    "        # score\n",
    "        mae_CVs.append(round(mean_absolute_error(y_test, y_hat), 3))\n",
    "        rmse_CVs.append(round(mean_squared_error(y_test, y_hat, square_root=True), 3))\n",
    "        mape_CVs.append(round(mean_absolute_percentage_error(y_test, y_hat), 3))\n",
    "        mase_CVs.append(round(mean_absolute_scaled_error(y_test, y_hat, y_train=y_train), 3))\n",
    "        \n",
    "    return {'store':model_name,\n",
    "        'mae_XGB':np.mean(mae_CVs),\n",
    "        'rmse_XGB':np.mean(rmse_CVs),\n",
    "        'mape_XGB':np.mean(mape_CVs),\n",
    "        'mase_XGB':np.mean(mase_CVs),\n",
    "        'fc_XGB':y_hat,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_store = pd.read_pickle(\"data/df_daily.pkl\")\n",
    "df_store['sales'] = df_store['sales']/1e6\n",
    "df_exog = pd.read_pickle(\"data/df_exog.pkl\")\n",
    "ts_company = df_store.groupby(\"date\").sum()[\"sales\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# horizon = 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tune on company data\n",
    "grid search:\n",
    "- fit once per param set on train set\n",
    "- predict & score on test set (no CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models compared: 729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid:   0%|                                               | 0/1 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kf/st4z7vm96nz8c7npjz8kjxt40000gp/T/ipykernel_49188/805142903.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Grid search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m results_grid = grid_search_forecaster(\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sales\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0minitial_train_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhorizon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive - Norges Handelshøyskole/Thesis/Sales_forecast/ap8venv/lib/python3.8/site-packages/skforecast/model_selection/model_selection.py\u001b[0m in \u001b[0;36mgrid_search_forecaster\u001b[0;34m(forecaster, y, param_grid, initial_train_size, steps, metric, exog, lags_grid, refit, return_best, verbose)\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m             \u001b[0mforecaster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m             metrics = backtesting_forecaster(\n\u001b[0m\u001b[1;32m   1036\u001b[0m                             \u001b[0mforecaster\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0mforecaster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m                             \u001b[0my\u001b[0m                  \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive - Norges Handelshøyskole/Thesis/Sales_forecast/ap8venv/lib/python3.8/site-packages/skforecast/model_selection/model_selection.py\u001b[0m in \u001b[0;36mbacktesting_forecaster\u001b[0;34m(forecaster, y, steps, metric, initial_train_size, exog, refit, interval, n_boot, random_state, in_sample_residuals, verbose, set_out_sample_residuals)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrefit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m         metric_value, backtest_predictions = _backtesting_forecaster_refit(\n\u001b[0m\u001b[1;32m    912\u001b[0m             \u001b[0mforecaster\u001b[0m          \u001b[0;34m=\u001b[0m \u001b[0mforecaster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[0my\u001b[0m                   \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive - Norges Handelshøyskole/Thesis/Sales_forecast/ap8venv/lib/python3.8/site-packages/skforecast/model_selection/model_selection.py\u001b[0m in \u001b[0;36m_backtesting_forecaster_refit\u001b[0;34m(forecaster, y, steps, metric, initial_train_size, exog, interval, n_boot, random_state, in_sample_residuals, verbose, set_out_sample_residuals)\u001b[0m\n\u001b[1;32m    425\u001b[0m                         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforecaster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m                         \u001b[0mforecaster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m                         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforecaster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnext_window_exog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive - Norges Handelshøyskole/Thesis/Sales_forecast/ap8venv/lib/python3.8/site-packages/skforecast/ForecasterAutoreg/ForecasterAutoreg.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y, exog)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregressor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"<class 'xgboost.sklearn.XGBRegressor'>\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive - Norges Handelshøyskole/Thesis/Sales_forecast/ap8venv/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive - Norges Handelshøyskole/Thesis/Sales_forecast/ap8venv/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive - Norges Handelshøyskole/Thesis/Sales_forecast/ap8venv/lib/python3.8/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_configure_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m         self._Booster = train(\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive - Norges Handelshøyskole/Thesis/Sales_forecast/ap8venv/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0mBooster\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mbooster\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \"\"\"\n\u001b[0;32m--> 188\u001b[0;31m     bst = _train_internal(params, dtrain,\n\u001b[0m\u001b[1;32m    189\u001b[0m                           \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                           \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive - Norges Handelshøyskole/Thesis/Sales_forecast/ap8venv/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Copy to serialise and unserialise booster to reset state and free\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m# training memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive - Norges Handelshøyskole/Thesis/Sales_forecast/ap8venv/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1531\u001b[0m             \u001b[0ma\u001b[0m \u001b[0mcopied\u001b[0m \u001b[0mbooster\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1532\u001b[0m         \"\"\"\n\u001b[0;32m-> 1533\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__copy__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1535\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive - Norges Handelshøyskole/Thesis/Sales_forecast/ap8venv/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__copy__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1518\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__copy__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1519\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__deepcopy__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__deepcopy__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive - Norges Handelshøyskole/Thesis/Sales_forecast/ap8venv/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__deepcopy__\u001b[0;34m(self, _)\u001b[0m\n\u001b[1;32m   1521\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__deepcopy__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;34m'''Return a copy of booster.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1523\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1525\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive - Norges Handelshøyskole/Thesis/Sales_forecast/ap8venv/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, cache, model_file)\u001b[0m\n\u001b[1;32m   1315\u001b[0m             \u001b[0;31m# We use the pickle interface for getting memory snapshot from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;31m# another model, and load the snapshot with this booster.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'handle'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'handle'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive - Norges Handelshøyskole/Thesis/Sales_forecast/ap8venv/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__getstate__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1431\u001b[0m             \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_bst_ulong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m             \u001b[0mcptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOINTER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m             _check_call(_LIB.XGBoosterSerializeToBuffer(self.handle,\n\u001b[0m\u001b[1;32m   1434\u001b[0m                                                         \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m                                                         ctypes.byref(cptr)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "horizon = 7\n",
    "\n",
    "# Grid search hyperparameter and lags\n",
    "# ==============================================================================\n",
    "pipe = make_pipeline(Normalizer(), XGBRegressor(random_state=123))\n",
    "\n",
    "forecaster = ForecasterAutoreg(\n",
    "    regressor=pipe, lags=10  # This value will be replaced in the grid search\n",
    ")\n",
    "\n",
    "# Regressor hyperparameters\n",
    "# param_grid = {\n",
    "#     \"xgbregressor__n_estimators\": [50],\n",
    "#     \"xgbregressor__max_depth\": [5],\n",
    "# } \n",
    "param_grid = {\n",
    "    \"xgbregressor__max_depth\": [3, 10, 20],\n",
    "    \"xgbregressor__learning_rate\": [0.01, 0.1, 0.3],\n",
    "    \"xgbregressor__subsample\": [0.5, 0.7, 1.0],\n",
    "    \"xgbregressor__colsample_bytree\": [0.5, 0.7, 1.0],\n",
    "    \"xgbregressor__colsample_bylevel\": [0.5, 0.7, 1.0],\n",
    "    \"xgbregressor__n_estimators\": [100, 500, 1000],\n",
    "}\n",
    "\n",
    "# Lags used as predictors\n",
    "lags_grid = [7]\n",
    "\n",
    "# Prepare data\n",
    "df, transformer = preprocessing(ts_company, df_exog, horizon)\n",
    "\n",
    "# Grid search\n",
    "results_grid = grid_search_forecaster(\n",
    "    y=df[\"sales\"],\n",
    "    initial_train_size=len(df) - horizon,\n",
    "    exog=df[df.columns.difference([\"sales\"])],\n",
    "    forecaster=forecaster,\n",
    "    param_grid=param_grid,\n",
    "    lags_grid=lags_grid,\n",
    "    steps=horizon,\n",
    "    refit=True,\n",
    "    metric=\"mean_absolute_percentage_error\",\n",
    "    return_best=True,\n",
    "    verbose=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import dump, load\n",
    "# # Save model\n",
    "# dump(forecaster, filename='results/f8/XGB_forecaster_7.py')\n",
    "\n",
    "# # Load model\n",
    "# forecaster = load('results/f8/XGB_forecaster_7.py')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit on store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "processing stores 307222...\n",
      "\n",
      "processing stores 307244...\n",
      "\n",
      "processing stores 307248...\n",
      "\n",
      "processing stores 320264...\n",
      "\n",
      "processing stores 328165...\n",
      "\n",
      "processing stores 349920...\n",
      "\n",
      "processing stores 349924...\n",
      "\n",
      "processing stores 349952...\n",
      "\n",
      "processing stores 349958...\n",
      "\n",
      "processing stores 349962...\n",
      "\n",
      "processing stores 349972...\n",
      "\n",
      "processing stores 349978...\n",
      "\n",
      "processing stores 349980...\n",
      "\n",
      "processing stores 349998...\n",
      "\n",
      "processing stores 350016...\n",
      "\n",
      "processing stores 350018...\n",
      "\n",
      "processing stores 350026...\n",
      "\n",
      "processing stores 350028...\n",
      "\n",
      "processing stores 350040...\n",
      "\n",
      "processing stores 350046...\n",
      "\n",
      "processing stores 350054...\n",
      "\n",
      "processing stores 350056...\n",
      "\n",
      "processing stores 350060...\n",
      "\n",
      "processing stores 354468...\n",
      "\n",
      "processing stores 387240...\n",
      "\n",
      "processing stores 412585...\n",
      "\n",
      "processing stores 441997...\n",
      "\n",
      "processing stores 452387...\n",
      "\n",
      "processing stores 461349...\n",
      "\n",
      "processing stores 464495...\n",
      "\n",
      "processing stores 471477...\n",
      "\n",
      "processing stores 476061...\n",
      "\n",
      "processing stores 480733...\n",
      "\n",
      "processing stores 528854...\n",
      "\n",
      "processing stores 536898...\n",
      "\n",
      "processing stores 536902...\n",
      "\n",
      "processing stores 566790...\n",
      "\n",
      "processing stores 566792...\n"
     ]
    }
   ],
   "source": [
    "rolls = 4\n",
    "all_stores_result_CV = []\n",
    "\n",
    "for store in df_store[\"store_id\"].unique():\n",
    "    # for store in df_store[\"store_id\"].unique()[:4]:\n",
    "    print(f\"\\nprocessing stores {store}...\")\n",
    "    model_name = \"store_\" + str(store)\n",
    "\n",
    "    # data\n",
    "    ts_1_store = df_store[df_store[\"store_id\"] == store].set_index(\"date\")[\"sales\"]\n",
    "    df_1_store_pro, transformer = preprocessing(\n",
    "        ts_1_store, \n",
    "        df_exog, \n",
    "        horizon=horizon * rolls\n",
    "    )\n",
    "\n",
    "    # CV\n",
    "    cv_score = cross_validation_result(\n",
    "        df_1_store_pro, \n",
    "        forecaster, \n",
    "        model_name, \n",
    "        transformer,\n",
    "        horizon,\n",
    "        rolls\n",
    "    )\n",
    "\n",
    "    # result\n",
    "    all_stores_result_CV.append(cv_score)\n",
    "all_stores_result_CV = pd.DataFrame(all_stores_result_CV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_stores_result_CV.to_pickle('results/f8/XGB_result_7.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# horizon = 14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tune on company data\n",
    "grid search:\n",
    "- fit once per param set on train set\n",
    "- predict & score on test set (no CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models compared: 729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid: 100%|█████████████████████████████████████| 1/1 [39:39<00:00, 2379.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [1 2 3 4 5 6 7] \n",
      "  Parameters: {'xgbregressor__colsample_bylevel': 1.0, 'xgbregressor__colsample_bytree': 1.0, 'xgbregressor__learning_rate': 0.01, 'xgbregressor__max_depth': 10, 'xgbregressor__n_estimators': 1000, 'xgbregressor__subsample': 0.5}\n",
      "  Backtesting metric: 0.07025408056191794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "horizon = 14\n",
    "\n",
    "# Grid search hyperparameter and lags\n",
    "# ==============================================================================\n",
    "pipe = make_pipeline(Normalizer(), XGBRegressor(random_state=123))\n",
    "\n",
    "forecaster = ForecasterAutoreg(\n",
    "    regressor=pipe, lags=10  # This value will be replaced in the grid search\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"xgbregressor__max_depth\": [3, 10, 20],\n",
    "    \"xgbregressor__learning_rate\": [0.01, 0.1, 0.3],\n",
    "    \"xgbregressor__subsample\": [0.5, 0.7, 1.0],\n",
    "    \"xgbregressor__colsample_bytree\": [0.5, 0.7, 1.0],\n",
    "    \"xgbregressor__colsample_bylevel\": [0.5, 0.7, 1.0],\n",
    "    \"xgbregressor__n_estimators\": [100, 500, 1000],\n",
    "}\n",
    "\n",
    "# Lags used as predictors\n",
    "lags_grid = [7]\n",
    "\n",
    "# Prepare data\n",
    "df, transformer = preprocessing(ts_company, df_exog, horizon)\n",
    "\n",
    "# Grid search\n",
    "results_grid = grid_search_forecaster(\n",
    "    y=df[\"sales\"],\n",
    "    initial_train_size=len(df) - horizon,\n",
    "    exog=df[df.columns.difference([\"sales\"])],\n",
    "    forecaster=forecaster,\n",
    "    param_grid=param_grid,\n",
    "    lags_grid=lags_grid,\n",
    "    steps=horizon,\n",
    "    refit=True,\n",
    "    metric=\"mean_absolute_percentage_error\",\n",
    "    return_best=True,\n",
    "    verbose=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import dump, load\n",
    "# # Save model\n",
    "# dump(forecaster, filename='results/f8/XGB_forecaster_14.py')\n",
    "\n",
    "# # Load model\n",
    "# forecaster = load('results/f8/XGB_forecaster_14.py')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit on store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "processing stores 307222...\n",
      "\n",
      "processing stores 307244...\n",
      "\n",
      "processing stores 307248...\n",
      "\n",
      "processing stores 320264...\n",
      "\n",
      "processing stores 328165...\n",
      "\n",
      "processing stores 349920...\n",
      "\n",
      "processing stores 349924...\n",
      "\n",
      "processing stores 349952...\n",
      "\n",
      "processing stores 349958...\n",
      "\n",
      "processing stores 349962...\n",
      "\n",
      "processing stores 349972...\n",
      "\n",
      "processing stores 349978...\n",
      "\n",
      "processing stores 349980...\n",
      "\n",
      "processing stores 349998...\n",
      "\n",
      "processing stores 350016...\n",
      "\n",
      "processing stores 350018...\n",
      "\n",
      "processing stores 350026...\n",
      "\n",
      "processing stores 350028...\n",
      "\n",
      "processing stores 350040...\n",
      "\n",
      "processing stores 350046...\n",
      "\n",
      "processing stores 350054...\n",
      "\n",
      "processing stores 350056...\n",
      "\n",
      "processing stores 350060...\n",
      "\n",
      "processing stores 354468...\n",
      "\n",
      "processing stores 387240...\n",
      "\n",
      "processing stores 412585...\n",
      "\n",
      "processing stores 441997...\n",
      "\n",
      "processing stores 452387...\n",
      "\n",
      "processing stores 461349...\n",
      "\n",
      "processing stores 464495...\n",
      "\n",
      "processing stores 471477...\n",
      "\n",
      "processing stores 476061...\n",
      "\n",
      "processing stores 480733...\n",
      "\n",
      "processing stores 528854...\n",
      "\n",
      "processing stores 536898...\n",
      "\n",
      "processing stores 536902...\n",
      "\n",
      "processing stores 566790...\n",
      "\n",
      "processing stores 566792...\n"
     ]
    }
   ],
   "source": [
    "rolls = 4\n",
    "all_stores_result_CV = []\n",
    "\n",
    "for store in df_store[\"store_id\"].unique():\n",
    "    # for store in df_store[\"store_id\"].unique()[:4]:\n",
    "    # print(f\"\\nprocessing stores {store}...\")\n",
    "    model_name = \"store_\" + str(store)\n",
    "\n",
    "    # data\n",
    "    ts_1_store = df_store[df_store[\"store_id\"] == store].set_index(\"date\")[\"sales\"]\n",
    "    df_1_store_pro, transformer = preprocessing(\n",
    "        ts_1_store, \n",
    "        df_exog, \n",
    "        horizon=horizon * rolls\n",
    "    )\n",
    "\n",
    "    # CV\n",
    "    cv_score = cross_validation_result(\n",
    "        df_1_store_pro, \n",
    "        forecaster, \n",
    "        model_name, \n",
    "        transformer,\n",
    "        horizon,\n",
    "        rolls\n",
    "    )\n",
    "\n",
    "    # result\n",
    "    all_stores_result_CV.append(cv_score)\n",
    "all_stores_result_CV = pd.DataFrame(all_stores_result_CV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_stores_result_CV.to_pickle('results/f8/XGB_result_14.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# horizon = 21\n",
    "\n",
    "## tune on company data\n",
    "grid search:\n",
    "- fit once per param set on train set\n",
    "- predict & score on test set (no CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models compared: 729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid: 100%|█████████████████████████████████████| 1/1 [51:44<00:00, 3104.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [1 2 3 4 5 6 7] \n",
      "  Parameters: {'xgbregressor__colsample_bylevel': 1.0, 'xgbregressor__colsample_bytree': 0.7, 'xgbregressor__learning_rate': 0.1, 'xgbregressor__max_depth': 3, 'xgbregressor__n_estimators': 1000, 'xgbregressor__subsample': 0.5}\n",
      "  Backtesting metric: 0.12988486571050464\n",
      "\n"
     ]
    }
   ],
   "source": [
    "horizon = 21\n",
    "\n",
    "# Grid search hyperparameter and lags\n",
    "# ==============================================================================\n",
    "pipe = make_pipeline(Normalizer(), XGBRegressor(random_state=123))\n",
    "\n",
    "forecaster = ForecasterAutoreg(\n",
    "    regressor=pipe, lags=10  # This value will be replaced in the grid search\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"xgbregressor__max_depth\": [3, 10, 20],\n",
    "    \"xgbregressor__learning_rate\": [0.01, 0.1, 0.3],\n",
    "    \"xgbregressor__subsample\": [0.5, 0.7, 1.0],\n",
    "    \"xgbregressor__colsample_bytree\": [0.5, 0.7, 1.0],\n",
    "    \"xgbregressor__colsample_bylevel\": [0.5, 0.7, 1.0],\n",
    "    \"xgbregressor__n_estimators\": [100, 500, 1000],\n",
    "}\n",
    "\n",
    "# Lags used as predictors\n",
    "lags_grid = [7]\n",
    "\n",
    "# Prepare data\n",
    "df, transformer = preprocessing(ts_company, df_exog, horizon)\n",
    "\n",
    "# Grid search\n",
    "results_grid = grid_search_forecaster(\n",
    "    y=df[\"sales\"],\n",
    "    initial_train_size=len(df) - horizon,\n",
    "    exog=df[df.columns.difference([\"sales\"])],\n",
    "    forecaster=forecaster,\n",
    "    param_grid=param_grid,\n",
    "    lags_grid=lags_grid,\n",
    "    steps=horizon,\n",
    "    refit=True,\n",
    "    metric=\"mean_absolute_percentage_error\",\n",
    "    return_best=True,\n",
    "    verbose=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['results/f8/XGB_forecaster_21.py']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "# Save model\n",
    "dump(forecaster, filename='results/f8/XGB_forecaster_21.py')\n",
    "\n",
    "# # Load model\n",
    "# forecaster = load('results/f8/XGB_forecaster_21.py')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit on store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolls = 4\n",
    "all_stores_result_CV = []\n",
    "\n",
    "for store in df_store[\"store_id\"].unique():\n",
    "    # for store in df_store[\"store_id\"].unique()[:4]:\n",
    "    # print(f\"\\nprocessing stores {store}...\")\n",
    "    model_name = \"store_\" + str(store)\n",
    "\n",
    "    # data\n",
    "    ts_1_store = df_store[df_store[\"store_id\"] == store].set_index(\"date\")[\"sales\"]\n",
    "    df_1_store_pro, transformer = preprocessing(\n",
    "        ts_1_store, \n",
    "        df_exog, \n",
    "        horizon=horizon * rolls\n",
    "    )\n",
    "\n",
    "    # CV\n",
    "    cv_score = cross_validation_result(\n",
    "        df_1_store_pro, \n",
    "        forecaster, \n",
    "        model_name, \n",
    "        transformer,\n",
    "        horizon,\n",
    "        rolls\n",
    "    )\n",
    "\n",
    "    # result\n",
    "    all_stores_result_CV.append(cv_score)\n",
    "all_stores_result_CV = pd.DataFrame(all_stores_result_CV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stores_result_CV.to_pickle('results/f8/XGB_result_21.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# horizon = 28\n",
    "\n",
    "## tune on company data\n",
    "grid search:\n",
    "- fit once per param set on train set\n",
    "- predict & score on test set (no CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models compared: 729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid: 100%|█████████████████████████████████████| 1/1 [36:53<00:00, 2213.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [1 2 3 4 5 6 7] \n",
      "  Parameters: {'xgbregressor__colsample_bylevel': 0.7, 'xgbregressor__colsample_bytree': 0.5, 'xgbregressor__learning_rate': 0.1, 'xgbregressor__max_depth': 3, 'xgbregressor__n_estimators': 1000, 'xgbregressor__subsample': 1.0}\n",
      "  Backtesting metric: 0.3241006999570049\n",
      "\n"
     ]
    }
   ],
   "source": [
    "horizon = 28\n",
    "\n",
    "# Grid search hyperparameter and lags\n",
    "# ==============================================================================\n",
    "pipe = make_pipeline(Normalizer(), XGBRegressor(random_state=123))\n",
    "\n",
    "forecaster = ForecasterAutoreg(\n",
    "    regressor=pipe, lags=10  # This value will be replaced in the grid search\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"xgbregressor__max_depth\": [3, 10, 20],\n",
    "    \"xgbregressor__learning_rate\": [0.01, 0.1, 0.3],\n",
    "    \"xgbregressor__subsample\": [0.5, 0.7, 1.0],\n",
    "    \"xgbregressor__colsample_bytree\": [0.5, 0.7, 1.0],\n",
    "    \"xgbregressor__colsample_bylevel\": [0.5, 0.7, 1.0],\n",
    "    \"xgbregressor__n_estimators\": [100, 500, 1000],\n",
    "}\n",
    "\n",
    "# Lags used as predictors\n",
    "lags_grid = [7]\n",
    "\n",
    "# Prepare data\n",
    "df, transformer = preprocessing(ts_company, df_exog, horizon)\n",
    "\n",
    "# Grid search\n",
    "results_grid = grid_search_forecaster(\n",
    "    y=df[\"sales\"],\n",
    "    initial_train_size=len(df) - horizon,\n",
    "    exog=df[df.columns.difference([\"sales\"])],\n",
    "    forecaster=forecaster,\n",
    "    param_grid=param_grid,\n",
    "    lags_grid=lags_grid,\n",
    "    steps=horizon,\n",
    "    refit=True,\n",
    "    metric=\"mean_absolute_percentage_error\",\n",
    "    return_best=True,\n",
    "    verbose=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['results/f8/XGB_forecaster_28.py']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "# Save model\n",
    "dump(forecaster, filename='results/f8/XGB_forecaster_28.py')\n",
    "\n",
    "# # Load model\n",
    "# forecaster = load('results/f8/XGB_forecaster_28.py')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit on store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolls = 4\n",
    "all_stores_result_CV = []\n",
    "\n",
    "for store in df_store[\"store_id\"].unique():\n",
    "    # for store in df_store[\"store_id\"].unique()[:4]:\n",
    "    # print(f\"\\nprocessing stores {store}...\")\n",
    "    model_name = \"store_\" + str(store)\n",
    "\n",
    "    # data\n",
    "    ts_1_store = df_store[df_store[\"store_id\"] == store].set_index(\"date\")[\"sales\"]\n",
    "    df_1_store_pro, transformer = preprocessing(\n",
    "        ts_1_store, \n",
    "        df_exog, \n",
    "        horizon=horizon * rolls\n",
    "    )\n",
    "\n",
    "    # CV\n",
    "    cv_score = cross_validation_result(\n",
    "        df_1_store_pro, \n",
    "        forecaster, \n",
    "        model_name, \n",
    "        transformer,\n",
    "        horizon,\n",
    "        rolls\n",
    "    )\n",
    "\n",
    "    # result\n",
    "    all_stores_result_CV.append(cv_score)\n",
    "all_stores_result_CV = pd.DataFrame(all_stores_result_CV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stores_result_CV.to_pickle('results/f8/XGB_result_28.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
