{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from skforecast.model_selection import grid_search_forecaster\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from xgboost import XGBRegressor\n",
    "from sktime.performance_metrics.forecasting import (\n",
    "    mean_absolute_scaled_error,\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_squared_error,\n",
    ")\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from sktime.transformations.series.detrend import Deseasonalizer, Detrender\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(y, X, horizon):\n",
    "    y.index.freq='D'\n",
    "\n",
    "    # split\n",
    "    y_train, y_test = train_test_split(y, test_size=horizon, shuffle=False)\n",
    "\n",
    "    # transform (deseasonalize)\n",
    "    transformer = make_pipeline(Deseasonalizer(sp=7), Detrender())\n",
    "    y_train_trans = transformer.fit_transform(y_train)\n",
    "    y_train_trans.name = y.name\n",
    "    y_test_trans = transformer.transform(y_test)\n",
    "    y_test_trans.name = y.name\n",
    "    y_trans = pd.concat([y_train_trans, y_test_trans])\n",
    "\n",
    "    # join\n",
    "    df = X.join(y_trans).dropna()\n",
    "    \n",
    "    # extract exo. variables from date index\n",
    "    df['dayofweek'] = df.index.dayofweek\n",
    "    df['dayofmonth'] = df.index.day\n",
    "    df['dayofyear'] = df.index.dayofyear\n",
    "    df['weekofyear'] = df.index.isocalendar()['week']\n",
    "    df['month'] = df.index.month\n",
    "    df['quarter'] = df.index.quarter\n",
    "    df['year'] = df.index.year\n",
    "\n",
    "    # rolling mean\n",
    "    df['rolling_mean_2'] = df['sales'].rolling(2).mean()\n",
    "    df['rolling_mean_3'] = df['sales'].rolling(3).mean()\n",
    "    df['rolling_mean_4'] = df['sales'].rolling(4).mean()\n",
    "    df['rolling_mean_5'] = df['sales'].rolling(5).mean()\n",
    "    df['rolling_mean_6'] = df['sales'].rolling(6).mean()\n",
    "    df['rolling_mean_7'] = df['sales'].rolling(7).mean()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    return df, transformer\n",
    "\n",
    "\n",
    "# CV \n",
    "def cross_validation_result(data, tuned_model, model_name, transformer, horizon, rolls):\n",
    "    # score model with CV on store data\n",
    "    mae_CVs = []\n",
    "    rmse_CVs = []\n",
    "    mape_CVs = []\n",
    "    mase_CVs = []\n",
    "    for i in range(rolls):\n",
    "        # print(f\"fold {i}---------------\")\n",
    "        \n",
    "        # split data\n",
    "        y_train = data.iloc[: -(rolls - i) * horizon]\n",
    "        y_test = data.iloc[\n",
    "            np.r_[-(rolls - i) * horizon : -(rolls - i - 1) * horizon]]\n",
    "\n",
    "        # fit model\n",
    "        model = tuned_model\n",
    "        model.fit(\n",
    "            y    = y_train['sales'],\n",
    "            exog = y_train[y_train.columns.difference(['sales'])]\n",
    "        )\n",
    "        \n",
    "        # make forecast\n",
    "        y_hat = model.predict(\n",
    "                        steps = horizon,\n",
    "                        exog = y_test[y_test.columns.difference(['sales'])]\n",
    "                    )\n",
    "        y_hat = pd.Series(data=y_hat, index=y_test.index)\n",
    "        \n",
    "        # inverse\n",
    "        y_train = transformer.inverse_transform(y_train['sales'])\n",
    "        y_test = transformer.inverse_transform(y_test['sales'])\n",
    "        y_hat = transformer.inverse_transform(y_hat)\n",
    "                \n",
    "        # score\n",
    "        mae_CVs.append(round(mean_absolute_error(y_test, y_hat), 3))\n",
    "        rmse_CVs.append(round(mean_squared_error(y_test, y_hat, square_root=True), 3))\n",
    "        mape_CVs.append(round(mean_absolute_percentage_error(y_test, y_hat), 3))\n",
    "        mase_CVs.append(round(mean_absolute_scaled_error(y_test, y_hat, y_train=y_train), 3))\n",
    "        \n",
    "    return {'store':model_name,\n",
    "        'mae_XGB':np.mean(mae_CVs),\n",
    "        'rmse_XGB':np.mean(rmse_CVs),\n",
    "        'mape_XGB':np.mean(mape_CVs),\n",
    "        'mase_XGB':np.mean(mase_CVs),\n",
    "        'fc_XGB':y_hat,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_store = pd.read_pickle(\"data/df_daily.pkl\")\n",
    "df_store['sales'] = df_store['sales']/1e6\n",
    "df_exog = pd.read_pickle(\"data/df_exog.pkl\")\n",
    "ts_company = df_store.groupby(\"date\").sum()[\"sales\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouping_pipe(ts, col_name):\n",
    "    all_stores_result_CV = pd.DataFrame(columns=[col_name,'store','mape'])\n",
    "    for group in ts[col_name].unique():\n",
    "        # get aggregated data for each store_segment\n",
    "        ts_group = ts[ts[col_name] == group].groupby('date').sum()['sales']\n",
    "        horizon = 7\n",
    "        # Prepare data\n",
    "        df, transformer = preprocessing(ts_group, df_exog, horizon)\n",
    "        \n",
    "\n",
    "        # Grid search hyperparameter and lags\n",
    "        # ==============================================================================\n",
    "        pipe = make_pipeline(Normalizer(), XGBRegressor(random_state=123))\n",
    "\n",
    "        forecaster = ForecasterAutoreg(\n",
    "            regressor=pipe, lags=10  # This value will be replaced in the grid search\n",
    "        )\n",
    "\n",
    "        param_grid = {\n",
    "            \"xgbregressor__max_depth\": [3, 10, 20],\n",
    "            \"xgbregressor__learning_rate\": [0.01, 0.1, 0.3],\n",
    "            \"xgbregressor__subsample\": [0.5, 0.7, 1.0],\n",
    "            \"xgbregressor__colsample_bytree\": [0.5, 0.7, 1.0],\n",
    "            \"xgbregressor__colsample_bylevel\": [0.5, 0.7, 1.0],\n",
    "            \"xgbregressor__n_estimators\": [100, 500, 1000],\n",
    "        }\n",
    "\n",
    "        # Lags used as predictors\n",
    "        lags_grid = [list(range(horizon, horizon*2))]\n",
    "\n",
    "        # Grid search\n",
    "        results_grid = grid_search_forecaster(\n",
    "            y=df[\"sales\"],\n",
    "            initial_train_size=len(df) - horizon,\n",
    "            exog=df[df.columns.difference([\"sales\"])],\n",
    "            forecaster=forecaster,\n",
    "            param_grid=param_grid,\n",
    "            lags_grid=lags_grid,\n",
    "            steps=horizon,\n",
    "            refit=True,\n",
    "            metric=\"mean_absolute_percentage_error\",\n",
    "            return_best=True,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        stores_result_CV = []\n",
    "        rolls = 4\n",
    "        store_list=ts[ts[col_name]==group]['store_id'].unique()\n",
    "        for store in store_list:  # print(store)\n",
    "        # fit on store data\n",
    "            print(f\"processing stores {store}...\")\n",
    "            model_name = \"store_\" + str(store)\n",
    "\n",
    "            # data\n",
    "            ts_1_store = ts[ts[\"store_id\"] == store].set_index(\"date\")[\"sales\"]\n",
    "            df_1_store_pro, transformer = preprocessing(\n",
    "                ts_1_store, df_exog, horizon=horizon * rolls\n",
    "            )\n",
    "\n",
    "            # CV\n",
    "            cv_score = cross_validation_result(\n",
    "                df_1_store_pro, \n",
    "                forecaster, \n",
    "                model_name, \n",
    "                transformer,\n",
    "                horizon,\n",
    "                rolls\n",
    "            )\n",
    "\n",
    "            # result\n",
    "            stores_result_CV.append(cv_score['mape_XGB'])\n",
    "        all_stores_result_CV = pd.concat(\n",
    "                                    [all_stores_result_CV,\n",
    "                                    pd.DataFrame({col_name:group,'store':store_list,'mape':stores_result_CV})])\n",
    "    return all_stores_result_CV\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_store = pd.read_pickle(\"data/df_daily.pkl\")\n",
    "df_cluster=pd.read_pickle(\"results/grouping/store_cluster.pkl\")\n",
    "df_store=pd.merge(df_store, df_cluster, on=['store_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models compared: 729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid: 100%|█████████████████████████████████████| 1/1 [18:12<00:00, 1092.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 7  8  9 10 11 12 13] \n",
      "  Parameters: {'xgbregressor__colsample_bylevel': 0.5, 'xgbregressor__colsample_bytree': 1.0, 'xgbregressor__learning_rate': 0.1, 'xgbregressor__max_depth': 10, 'xgbregressor__n_estimators': 500, 'xgbregressor__subsample': 0.5}\n",
      "  Backtesting metric: 0.20661924638686222\n",
      "\n",
      "processing stores 307222...\n",
      "processing stores 307244...\n",
      "processing stores 307248...\n",
      "processing stores 320264...\n",
      "processing stores 328165...\n",
      "processing stores 349920...\n",
      "processing stores 349924...\n",
      "processing stores 349952...\n",
      "processing stores 349958...\n",
      "processing stores 349962...\n",
      "processing stores 349972...\n",
      "processing stores 349978...\n",
      "processing stores 349980...\n",
      "processing stores 452387...\n",
      "processing stores 461349...\n",
      "processing stores 480733...\n",
      "processing stores 528854...\n",
      "processing stores 566790...\n",
      "Number of models compared: 729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid: 100%|█████████████████████████████████████| 1/1 [18:21<00:00, 1101.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 7  8  9 10 11 12 13] \n",
      "  Parameters: {'xgbregressor__colsample_bylevel': 1.0, 'xgbregressor__colsample_bytree': 0.7, 'xgbregressor__learning_rate': 0.3, 'xgbregressor__max_depth': 10, 'xgbregressor__n_estimators': 500, 'xgbregressor__subsample': 0.7}\n",
      "  Backtesting metric: 0.6040432881621337\n",
      "\n",
      "processing stores 349998...\n",
      "Number of models compared: 729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid: 100%|█████████████████████████████████████| 1/1 [18:25<00:00, 1105.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 7  8  9 10 11 12 13] \n",
      "  Parameters: {'xgbregressor__colsample_bylevel': 0.7, 'xgbregressor__colsample_bytree': 1.0, 'xgbregressor__learning_rate': 0.1, 'xgbregressor__max_depth': 3, 'xgbregressor__n_estimators': 500, 'xgbregressor__subsample': 0.7}\n",
      "  Backtesting metric: 0.14813787321869767\n",
      "\n",
      "processing stores 350016...\n",
      "processing stores 350018...\n",
      "processing stores 441997...\n",
      "Number of models compared: 729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid: 100%|█████████████████████████████████████| 1/1 [18:56<00:00, 1136.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 7  8  9 10 11 12 13] \n",
      "  Parameters: {'xgbregressor__colsample_bylevel': 0.7, 'xgbregressor__colsample_bytree': 1.0, 'xgbregressor__learning_rate': 0.01, 'xgbregressor__max_depth': 3, 'xgbregressor__n_estimators': 100, 'xgbregressor__subsample': 0.5}\n",
      "  Backtesting metric: 0.48686173155734214\n",
      "\n",
      "processing stores 350026...\n",
      "Number of models compared: 729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid: 100%|█████████████████████████████████████| 1/1 [18:39<00:00, 1119.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 7  8  9 10 11 12 13] \n",
      "  Parameters: {'xgbregressor__colsample_bylevel': 1.0, 'xgbregressor__colsample_bytree': 1.0, 'xgbregressor__learning_rate': 0.1, 'xgbregressor__max_depth': 10, 'xgbregressor__n_estimators': 500, 'xgbregressor__subsample': 0.7}\n",
      "  Backtesting metric: 0.6838382472294987\n",
      "\n",
      "processing stores 350028...\n",
      "Number of models compared: 729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid: 100%|█████████████████████████████████████| 1/1 [19:10<00:00, 1150.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 7  8  9 10 11 12 13] \n",
      "  Parameters: {'xgbregressor__colsample_bylevel': 1.0, 'xgbregressor__colsample_bytree': 1.0, 'xgbregressor__learning_rate': 0.3, 'xgbregressor__max_depth': 10, 'xgbregressor__n_estimators': 500, 'xgbregressor__subsample': 0.5}\n",
      "  Backtesting metric: 0.5459120574178099\n",
      "\n",
      "processing stores 350040...\n",
      "processing stores 350046...\n",
      "Number of models compared: 729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid: 100%|█████████████████████████████████████| 1/1 [19:54<00:00, 1194.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 7  8  9 10 11 12 13] \n",
      "  Parameters: {'xgbregressor__colsample_bylevel': 0.5, 'xgbregressor__colsample_bytree': 0.5, 'xgbregressor__learning_rate': 0.1, 'xgbregressor__max_depth': 3, 'xgbregressor__n_estimators': 1000, 'xgbregressor__subsample': 0.5}\n",
      "  Backtesting metric: 1.356938363590233\n",
      "\n",
      "processing stores 350054...\n",
      "Number of models compared: 729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid: 100%|█████████████████████████████████████| 1/1 [22:09<00:00, 1329.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 7  8  9 10 11 12 13] \n",
      "  Parameters: {'xgbregressor__colsample_bylevel': 0.7, 'xgbregressor__colsample_bytree': 0.5, 'xgbregressor__learning_rate': 0.3, 'xgbregressor__max_depth': 10, 'xgbregressor__n_estimators': 100, 'xgbregressor__subsample': 0.7}\n",
      "  Backtesting metric: 0.6443367286013325\n",
      "\n",
      "processing stores 350056...\n",
      "Number of models compared: 729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid: 100%|█████████████████████████████████████| 1/1 [21:48<00:00, 1308.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 7  8  9 10 11 12 13] \n",
      "  Parameters: {'xgbregressor__colsample_bylevel': 0.7, 'xgbregressor__colsample_bytree': 0.5, 'xgbregressor__learning_rate': 0.3, 'xgbregressor__max_depth': 3, 'xgbregressor__n_estimators': 500, 'xgbregressor__subsample': 0.5}\n",
      "  Backtesting metric: 1.1111519985466216\n",
      "\n",
      "processing stores 350060...\n",
      "Number of models compared: 729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid: 100%|█████████████████████████████████████| 1/1 [21:30<00:00, 1290.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 7  8  9 10 11 12 13] \n",
      "  Parameters: {'xgbregressor__colsample_bylevel': 0.5, 'xgbregressor__colsample_bytree': 1.0, 'xgbregressor__learning_rate': 0.3, 'xgbregressor__max_depth': 3, 'xgbregressor__n_estimators': 1000, 'xgbregressor__subsample': 1.0}\n",
      "  Backtesting metric: 0.5545333794420225\n",
      "\n",
      "processing stores 354468...\n",
      "Number of models compared: 729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid: 100%|█████████████████████████████████████| 1/1 [19:46<00:00, 1186.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 7  8  9 10 11 12 13] \n",
      "  Parameters: {'xgbregressor__colsample_bylevel': 0.7, 'xgbregressor__colsample_bytree': 1.0, 'xgbregressor__learning_rate': 0.01, 'xgbregressor__max_depth': 3, 'xgbregressor__n_estimators': 100, 'xgbregressor__subsample': 0.7}\n",
      "  Backtesting metric: 1.298677032206252\n",
      "\n",
      "processing stores 387240...\n",
      "Number of models compared: 729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid: 100%|█████████████████████████████████████| 1/1 [19:44<00:00, 1184.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 7  8  9 10 11 12 13] \n",
      "  Parameters: {'xgbregressor__colsample_bylevel': 0.5, 'xgbregressor__colsample_bytree': 1.0, 'xgbregressor__learning_rate': 0.1, 'xgbregressor__max_depth': 20, 'xgbregressor__n_estimators': 1000, 'xgbregressor__subsample': 0.7}\n",
      "  Backtesting metric: 0.35727997386050053\n",
      "\n",
      "processing stores 412585...\n",
      "Number of models compared: 729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid: 100%|█████████████████████████████████████| 1/1 [18:06<00:00, 1086.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 7  8  9 10 11 12 13] \n",
      "  Parameters: {'xgbregressor__colsample_bylevel': 0.7, 'xgbregressor__colsample_bytree': 1.0, 'xgbregressor__learning_rate': 0.3, 'xgbregressor__max_depth': 20, 'xgbregressor__n_estimators': 100, 'xgbregressor__subsample': 0.7}\n",
      "  Backtesting metric: 0.5196548920455004\n",
      "\n",
      "processing stores 464495...\n",
      "Number of models compared: 729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid: 100%|█████████████████████████████████████| 1/1 [18:18<00:00, 1098.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 7  8  9 10 11 12 13] \n",
      "  Parameters: {'xgbregressor__colsample_bylevel': 1.0, 'xgbregressor__colsample_bytree': 0.5, 'xgbregressor__learning_rate': 0.3, 'xgbregressor__max_depth': 3, 'xgbregressor__n_estimators': 1000, 'xgbregressor__subsample': 0.5}\n",
      "  Backtesting metric: 0.3473484589896351\n",
      "\n",
      "processing stores 471477...\n",
      "Number of models compared: 729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid: 100%|█████████████████████████████████████| 1/1 [16:44<00:00, 1004.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 7  8  9 10 11 12 13] \n",
      "  Parameters: {'xgbregressor__colsample_bylevel': 0.7, 'xgbregressor__colsample_bytree': 0.7, 'xgbregressor__learning_rate': 0.3, 'xgbregressor__max_depth': 10, 'xgbregressor__n_estimators': 100, 'xgbregressor__subsample': 0.7}\n",
      "  Backtesting metric: 0.4164337239481904\n",
      "\n",
      "processing stores 476061...\n",
      "processing stores 566792...\n",
      "Number of models compared: 729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid: 100%|██████████████████████████████████████| 1/1 [14:05<00:00, 845.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 7  8  9 10 11 12 13] \n",
      "  Parameters: {'xgbregressor__colsample_bylevel': 0.7, 'xgbregressor__colsample_bytree': 0.7, 'xgbregressor__learning_rate': 0.1, 'xgbregressor__max_depth': 20, 'xgbregressor__n_estimators': 500, 'xgbregressor__subsample': 1.0}\n",
      "  Backtesting metric: 0.22139816181221647\n",
      "\n",
      "processing stores 536898...\n",
      "Number of models compared: 729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid: 100%|██████████████████████████████████████| 1/1 [13:45<00:00, 825.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 7  8  9 10 11 12 13] \n",
      "  Parameters: {'xgbregressor__colsample_bylevel': 1.0, 'xgbregressor__colsample_bytree': 0.7, 'xgbregressor__learning_rate': 0.3, 'xgbregressor__max_depth': 20, 'xgbregressor__n_estimators': 500, 'xgbregressor__subsample': 0.7}\n",
      "  Backtesting metric: 0.25979358990833373\n",
      "\n",
      "processing stores 536902...\n"
     ]
    }
   ],
   "source": [
    "#columns_list= ['store_level','store_segment','cluster']\n",
    "columns_list= ['province']\n",
    "for col in columns_list:\n",
    "    final_result=grouping_pipe(df_store, col)\n",
    "    file_dir = \"results/grouping/result_XGB_\"+ col + \".pkl\"\n",
    "    final_result.to_pickle(file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dc1c8500d1c7d68ad4fe3470dd61d80ea8045d933e624a9aa39b7580cf077167"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('DataMining_Lab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
